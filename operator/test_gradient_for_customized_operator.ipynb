{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to train a model\n",
    "def train_model(model, trainloader):\n",
    "    '''\n",
    "    Function trains the model and prints out the training log.\n",
    "    INPUT:\n",
    "        model - initialized PyTorch model ready for training.\n",
    "        trainloader - PyTorch dataloader for training data.\n",
    "    '''\n",
    "    #setup training\n",
    "\n",
    "    #define loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "    #define learning rate\n",
    "    learning_rate = 0.003\n",
    "    #define number of epochs\n",
    "    epochs = 5\n",
    "    #initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #run training and print out the loss to make sure that we are actually fitting to the training set\n",
    "    print('Training the model. Make sure that loss decreases after each epoch.\\n')\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).cuda()\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            # print out the loss to make sure it is decreasing\n",
    "            print(f\"Training loss: {running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply define a silu function\n",
    "def silu(input):\n",
    "    '''\n",
    "    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n",
    "        SiLU(x) = x * sigmoid(x)\n",
    "    '''\n",
    "    return input * torch.sigmoid(input) # use torch.sigmoid to make sure that we created the most efficient implemetation based on builtin PyTorch functions\n",
    "\n",
    "# create a class wrapper from PyTorch nn.Module, so\n",
    "# the function now can be easily used in models\n",
    "class SiLU(nn.Module):\n",
    "    '''\n",
    "    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n",
    "        SiLU(x) = x * sigmoid(x)\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    References:\n",
    "        -  Related paper:\n",
    "        https://arxiv.org/pdf/1606.08415.pdf\n",
    "    Examples:\n",
    "        >>> m = silu()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__() # init the base class\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return silu(input) # simply apply already implemented SiLU\n",
    "\n",
    "# create class for basic fully-connected deep neural network\n",
    "class ClassifierSiLU(nn.Module):\n",
    "    '''\n",
    "    Demo classifier model class to demonstrate SiLU\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure the input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # apply silu function\n",
    "        x = silu(self.fc1(x))\n",
    "\n",
    "        # apply silu function\n",
    "        x = silu(self.fc2(x))\n",
    "\n",
    "        # apply silu function\n",
    "        x = silu(self.fc3(x))\n",
    "\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Soft Exponential activation function\n",
    "class soft_exponential(nn.Module):\n",
    "    '''\n",
    "    Implementation of soft exponential activation.\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    Parameters:\n",
    "        - alpha - trainable parameter\n",
    "    References:\n",
    "        - See related paper:\n",
    "        https://arxiv.org/pdf/1602.01321.pdf\n",
    "    Examples:\n",
    "        >>> a1 = soft_exponential(256)\n",
    "        >>> x = torch.randn(256)\n",
    "        >>> x = a1(x)\n",
    "    '''\n",
    "    def __init__(self, in_features, alpha = None):\n",
    "        '''\n",
    "        Initialization.\n",
    "        INPUT:\n",
    "            - in_features: shape of the input\n",
    "            - aplha: trainable parameter\n",
    "            aplha is initialized with zero value by default\n",
    "        '''\n",
    "        super(soft_exponential,self).__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        # initialize alpha\n",
    "        if alpha == None:\n",
    "            self.alpha = Parameter(torch.tensor(0.0)) # create a tensor out of alpha\n",
    "        else:\n",
    "            self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha\n",
    "\n",
    "        self.alpha.requiresGrad = True # set requiresGrad to true!\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        Applies the function to the input elementwise.\n",
    "        '''\n",
    "        if (self.alpha == 0.0):\n",
    "            return x\n",
    "\n",
    "        if (self.alpha < 0.0):\n",
    "            return - torch.log(1 - self.alpha * (x + self.alpha)) / self.alpha\n",
    "\n",
    "        if (self.alpha > 0.0):\n",
    "            return (torch.exp(self.alpha * x) - 1)/ self.alpha + self.alpha\n",
    "\n",
    "# create class for basic fully-connected deep neural network\n",
    "class ClassifierSExp(nn.Module):\n",
    "    '''\n",
    "    Basic fully-connected network to test Soft Exponential activation.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # initialize Soft Exponential activation\n",
    "        self.a1 = soft_exponential(256)\n",
    "        self.a2 = soft_exponential(128)\n",
    "        self.a3 = soft_exponential(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure the input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # apply Soft Exponential unit\n",
    "        x = self.a1(self.fc1(x))\n",
    "        x = self.a2(self.fc2(x))\n",
    "        x = self.a3(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of BReLU activation function with custom backward step\n",
    "class brelu(Function):\n",
    "    '''\n",
    "    Implementation of BReLU activation function.\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    References:\n",
    "        - See BReLU paper:\n",
    "        https://arxiv.org/pdf/1709.04054.pdf\n",
    "    Examples:\n",
    "        >>> brelu_activation = brelu.apply\n",
    "        >>> t = torch.randn((5,5), dtype=torch.float, requires_grad = True)\n",
    "        >>> t = brelu_activation(t)\n",
    "    '''\n",
    "    #both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input) # save input for backward pass\n",
    "\n",
    "        # get lists of odd and even indices\n",
    "        input_shape = input.shape[0]\n",
    "        even_indices = [i for i in range(0, input_shape, 2)]\n",
    "        odd_indices = [i for i in range(1, input_shape, 2)]\n",
    "\n",
    "        # clone the input tensor\n",
    "        output = input.clone()\n",
    "\n",
    "        # apply ReLU to elements where i mod 2 == 0\n",
    "        output[even_indices] = output[even_indices].clamp(min=0)\n",
    "\n",
    "        # apply inversed ReLU to inversed elements where i mod 2 != 0\n",
    "        output[odd_indices] = 0 - output[odd_indices] # reverse elements with odd indices\n",
    "        output[odd_indices] = - output[odd_indices].clamp(min = 0) # apply reversed ReLU\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        grad_input = None # set output to None\n",
    "\n",
    "        input, = ctx.saved_tensors # restore input from context\n",
    "\n",
    "        # check that input requires grad\n",
    "        # if not requires grad we will return None to speed up computation\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.clone()\n",
    "\n",
    "            # get lists of odd and even indices\n",
    "            input_shape = input.shape[0]\n",
    "            even_indices = [i for i in range(0, input_shape, 2)]\n",
    "            odd_indices = [i for i in range(1, input_shape, 2)]\n",
    "\n",
    "            # set grad_input for even_indices\n",
    "            grad_input[even_indices] = (input[even_indices] >= 0).float() * grad_input[even_indices]\n",
    "\n",
    "            # set grad_input for odd_indices\n",
    "            grad_input[odd_indices] = (input[odd_indices] < 0).float() * grad_input[odd_indices]\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "# Simple model to demonstrate BReLU\n",
    "class ClassifierBReLU(nn.Module):\n",
    "    '''\n",
    "    Simple fully-connected classifier model to demonstrate BReLU activation.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ClassifierBReLU, self).__init__()\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # create shortcuts for BReLU\n",
    "        self.a1 = brelu.apply\n",
    "        self.a2 = brelu.apply\n",
    "        self.a3 = brelu.apply\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure the input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # apply BReLU\n",
    "        x = self.a1(self.fc1(x))\n",
    "        x = self.a2(self.fc2(x))\n",
    "        x = self.a3(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Fasion MNIST dataset.\n",
      "\n",
      "Training model with SiLU activation.\n",
      "\n",
      "Training the model. Make sure that loss decreases after each epoch.\n",
      "\n",
      "Training loss: 456.2534524202347\n",
      "Training loss: 345.29956662654877\n",
      "Training loss: 313.46922321617603\n",
      "Training loss: 291.3831397071481\n",
      "Training loss: 275.10592034459114\n",
      "Training model with SiLU activation.\n",
      "\n",
      "Training the model. Make sure that loss decreases after each epoch.\n",
      "\n",
      "Training loss: 465.99914529919624\n",
      "Training loss: 342.34088199585676\n",
      "Training loss: 309.9063211977482\n",
      "Training loss: 286.5664359629154\n",
      "Training loss: 272.5966087281704\n",
      "Training model with Soft Exponential activation.\n",
      "\n",
      "Training the model. Make sure that loss decreases after each epoch.\n",
      "\n",
      "Training loss: 551.9672828018665\n",
      "Training loss: 476.7195133715868\n",
      "Training loss: 449.98978397250175\n",
      "Training loss: 434.2893971055746\n",
      "Training loss: 428.93401578068733\n",
      "Training model with BReLU activation.\n",
      "\n",
      "Training the model. Make sure that loss decreases after each epoch.\n",
      "\n",
      "Training loss: 575.5205841809511\n",
      "Training loss: 434.76817102730274\n",
      "Training loss: 406.8466499000788\n",
      "Training loss: 378.75924730300903\n",
      "Training loss: 366.0211959704757\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print('Loading the Fasion MNIST dataset.\\n')\n",
    "\n",
    "    # Define a transform\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # Download and load the training data for Fashion MNIST\n",
    "    trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # 1. SiLU demonstration with model created with Sequential\n",
    "    # use SiLU with model created with Sequential\n",
    "\n",
    "    # initialize activation function\n",
    "    activation_function = SiLU()\n",
    "\n",
    "    # Initialize the model using nn.Sequential\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(784, 256)),\n",
    "                          ('activation1', activation_function), # use SiLU\n",
    "                          ('fc2', nn.Linear(256, 128)),\n",
    "                          ('bn2', nn.BatchNorm1d(num_features=128)),\n",
    "                          ('activation2', activation_function), # use SiLU\n",
    "                          ('dropout', nn.Dropout(0.3)),\n",
    "                          ('fc3', nn.Linear(128, 64)),\n",
    "                          ('bn3', nn.BatchNorm1d(num_features=64)),\n",
    "                          ('activation3', activation_function), # use SiLU\n",
    "                          ('logits', nn.Linear(64, 10)),\n",
    "                          ('logsoftmax', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "    # Run training\n",
    "    print('Training model with SiLU activation.\\n')\n",
    "    train_model(model.cuda(), trainloader)\n",
    "\n",
    "    # 2. SiLU demonstration of a model defined as a nn.Module\n",
    "\n",
    "    # Create demo model\n",
    "    model = ClassifierSiLU()\n",
    "\n",
    "    # Run training\n",
    "    print('Training model with SiLU activation.\\n')\n",
    "    train_model(model.cuda(), trainloader)\n",
    "\n",
    "    # 3. Soft Eponential function demonstration (with trainable parameter alpha)\n",
    "\n",
    "    # Create demo model\n",
    "    model = ClassifierSExp()\n",
    "\n",
    "    # Run training\n",
    "    print('Training model with Soft Exponential activation.\\n')\n",
    "    train_model(model.cuda(), trainloader)\n",
    "\n",
    "    # 4. BReLU activation function demonstration (with custom backward step)\n",
    "\n",
    "    # Create demo model\n",
    "    model = ClassifierBReLU()\n",
    "\n",
    "    # Run training\n",
    "    print('Training model with BReLU activation.\\n')\n",
    "    train_model(model.cuda(), trainloader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
